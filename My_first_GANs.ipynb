{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport errno\nimport torchvision.utils as vutils\nfrom torchvision import transforms, datasets\nfrom tensorboardX import SummaryWriter\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.autograd import Variable\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.is_available(), device","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Logger:\n\n    def __init__(self, model_name, data_name):\n        self.model_name = model_name\n        self.data_name = data_name\n\n        self.comment = f'{model_name}_{data_name}'\n        self.data_subdir = f'{model_name}/{data_name}'\n\n        # TensorBoard\n        self.writer = SummaryWriter(comment=self.comment)\n\n    def log(self, d_error, g_error, epoch, n_batch, num_batches):\n\n        # var_class = torch.autograd.variable.Variable\n        if isinstance(d_error, torch.autograd.Variable):\n            d_error = d_error.data.cpu().numpy()\n        if isinstance(g_error, torch.autograd.Variable):\n            g_error = g_error.data.cpu().numpy()\n\n        step = Logger._step(epoch, n_batch, num_batches)\n        self.writer.add_scalar(f'{self.comment}/D_error', d_error, step)\n        self.writer.add_scalar(f'{self.comment}/G_error', g_error, step)\n\n    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n        '''\n        input images are expected in format (NCHW)\n        '''\n        if type(images) == np.ndarray:\n            images = torch.from_numpy(images)\n        \n        if format=='NHWC':\n            images = images.transpose(1, 3)\n        \n\n        step = Logger._step(epoch, n_batch, num_batches)\n        img_name = f'{self.comment}/images'\n\n        # Make horizontal grid from image tensor\n        horizontal_grid = vutils.make_grid(images, normalize=normalize, scale_each=True)\n        # Make vertical grid from image tensor\n        nrows = int(np.sqrt(num_images))\n        grid = vutils.make_grid(images, nrow=nrows, normalize=True, scale_each=True)\n\n        # Add horizontal images to tensorboard\n        self.writer.add_image(img_name, horizontal_grid, step)\n\n        # Save plots\n        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)\n\n    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n        out_dir = f'./data/images/{self.data_subdir}'\n        Logger._make_dir(out_dir)\n\n        # Plot and save horizontal\n        fig = plt.figure(figsize=(16, 16))\n        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n        plt.axis('off')\n        if plot_horizontal:\n            display.display(plt.gcf())\n        self._save_images(fig, epoch, n_batch, 'hori')\n        plt.close()\n\n        # Save squared\n        fig = plt.figure()\n        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n        plt.axis('off')\n        self._save_images(fig, epoch, n_batch)\n        plt.close()\n\n    def _save_images(self, fig, epoch, n_batch, comment=''):\n        out_dir = f'./data/images/{self.data_subdir}'\n        Logger._make_dir(out_dir)\n        fig.savefig(f'{out_dir}/{comment}_epoch_{epoch}_batch_{n_batch}.png')\n\n    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):\n        # var_class = torch.autograd.variable.Variable\n        if isinstance(d_error, torch.autograd.Variable):\n            d_error = d_error.data.cpu().numpy()\n        if isinstance(g_error, torch.autograd.Variable):\n            g_error = g_error.data.cpu().numpy()\n        if isinstance(d_pred_real, torch.autograd.Variable):\n            d_pred_real = d_pred_real.data\n        if isinstance(d_pred_fake, torch.autograd.Variable):\n            d_pred_fake = d_pred_fake.data\n\n        print(f'Epoch: [{epoch}/{num_epochs}], Batch Num: [{n_batch}/{num_batches}]')\n        print(f'Discriminator Loss: {d_error:.4f}, Generator Loss: {g_error:.4f}')\n        print(f'D(x): {d_pred_real.mean():.4f}, D(G(z)): {d_pred_fake.mean():.4f}')\n\n    def save_models(self, generator, discriminator, epoch):\n        out_dir = f'../working/models/{self.data_subdir}'\n        Logger._make_dir(out_dir)\n        torch.save(generator.state_dict(), f'{out_dir}/G_epoch_{epoch}')\n        torch.save(discriminator.state_dict(), f'{out_dir}/D_epoch_{epoch}')\n\n    def close(self):\n        self.writer.close()\n\n    # Private Functionality\n    @staticmethod\n    def _step(epoch, n_batch, num_batches):\n        return epoch * num_batches + n_batch\n\n    @staticmethod\n    def _make_dir(directory):\n        try:\n            os.makedirs(directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mnist_data():\n    compose = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n        ])\n    out_dir = '../working'\n    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n\n# Load data\ndata = mnist_data()\n# Create loader with data, so that we can iterate over it\ndata_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n# Num batches\nnum_batches = len(data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiscriminatorNet(torch.nn.Module):\n    \"\"\"\n    A three hidden-layer discriminative neural network\n    \"\"\"\n    def __init__(self):\n        super(DiscriminatorNet, self).__init__()\n        n_features = 784\n        n_out = 1\n        \n        self.hidden0 = nn.Sequential( \n            nn.Linear(n_features, 1024),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3)\n        )\n        self.hidden1 = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3)\n        )\n        self.hidden2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3)\n        )\n        self.out = nn.Sequential(\n            torch.nn.Linear(256, n_out),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.hidden0(x)\n        x = self.hidden1(x)\n        x = self.hidden2(x)\n        x = self.out(x)\n        return x\n\ndiscriminator = DiscriminatorNet()\n# discriminator = discriminator.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def images_to_vectors(images):\n    return images.view(images.size(0), 784)\n\ndef vectors_to_images(vectors):\n    return vectors.view(vectors.size(0), 1, 28, 28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GeneratorNet(torch.nn.Module):\n    \"\"\"\n    A three hidden-layer generative neural network\n    \"\"\"\n    def __init__(self):\n        super(GeneratorNet, self).__init__()\n        n_features = 100\n        n_out = 784\n        \n        self.hidden0 = nn.Sequential(\n            nn.Linear(n_features, 256),\n            nn.LeakyReLU(0.2)\n        )\n        self.hidden1 = nn.Sequential(            \n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2)\n        )\n        self.hidden2 = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2)\n        )\n        \n        self.out = nn.Sequential(\n            nn.Linear(1024, n_out),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.hidden0(x)\n        x = self.hidden1(x)\n        x = self.hidden2(x)\n        x = self.out(x)\n        return x\n\ngenerator = GeneratorNet()\n# generator = generator.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def noise(size):\n    '''\n    Generates a 1-d vector of gaussian sampled random values\n    '''\n    n = Variable(torch.randn(size, 100))\n    return n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ng_optimizer = optim.Adam(generator.parameters(), lr=0.0002)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ones_target(size):\n    '''\n    Tensor containing ones, with shape = size\n    '''\n    data = Variable(torch.ones(size, 1))\n#   data = data.to(device)\n    return data\n\ndef zeros_target(size):\n    '''\n    Tensor containing zeros, with shape = size\n    '''\n    data = Variable(torch.zeros(size, 1))\n#   data = data.to(device)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_discriminator(optimizer, real_data, fake_data):\n    N = real_data.size(0)\n    # Reset gradients\n    optimizer.zero_grad()\n    \n    # 1.1 Train on Real Data\n    prediction_real = discriminator(real_data)\n    # Calculate error and backpropagate\n    error_real = loss(prediction_real, ones_target(N))\n    error_real.backward()\n\n    # 1.2 Train on Fake Data\n    prediction_fake = discriminator(fake_data)\n    # Calculate error and backpropagate\n    error_fake = loss(prediction_fake, zeros_target(N))\n    error_fake.backward()\n    \n    # 1.3 Update weights with gradients\n    optimizer.step()\n    \n    # Return error and predictions for real and fake inputs\n    return error_real + error_fake, prediction_real, prediction_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(optimizer, fake_data):\n    N = fake_data.size(0)\n    # Reset gradients\n    optimizer.zero_grad()\n    # Sample noise and generate fake data\n    prediction = discriminator(fake_data)\n    # Calculate error and backpropagate\n    error = loss(prediction, ones_target(N))\n    error.backward()\n    # Update weights with gradients\n    optimizer.step()\n    # Return error\n    return error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_test_samples = 8\ntest_noise = noise(num_test_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create logger instance\nlogger = Logger(model_name='VGAN', data_name='MNIST')\n# Total number of epochs to train\nnum_epochs = 2\nfor epoch in range(num_epochs):\n    for n_batch, (real_batch,_) in enumerate(data_loader):\n        N = real_batch.size(0)\n        \n        # 1. Train Discriminator\n        real_data = Variable(images_to_vectors(real_batch))\n#         real_data = real_data.to(device)\n        \n        # Generate fake data and detach \n        # (so gradients are not calculated for generator)\n        fake_data = generator(noise(N)).detach()\n#         fake_data = fake_data.to(device)\n        \n        # Train D\n        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, real_data, fake_data)\n\n        # 2. Train Generator\n        # Generate fake data\n        fake_data = generator(noise(N))\n        \n        # Train G\n        g_error = train_generator(g_optimizer, fake_data)\n        # Log batch error\n        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n        # Display Progress every few batches\n        if num_epochs % 10 == 0 and n_batch == 0:\n            test_images = vectors_to_images(generator(test_noise))\n            test_images = test_images.data\n            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n            # Display status Logs\n            logger.display_status(epoch, num_epochs, n_batch, num_batches,\n                                  d_error, g_error, d_pred_real, d_pred_fake)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def mnist_data():\n#     compose = transforms.Compose([\n#         transforms.ToTensor(),\n#         transforms.Normalize((0.5,), (0.5,))\n#         ])\n#     out_dir = '../working'\n#     return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n\n# # Load data\n# data = mnist_data()\n# # Create loader with data, so that we can iterate over it\n# data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n# # Num batches\n# num_batches = len(data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class GAN():\n#     def __init__(self):\n#         self.img_rows = 28\n#         self.img_cols = 28\n#         self.channels = 1\n#         self.img_shape = (self.img_rows, self.img_cols, self.channels)\n\n#         optimizer = torch.optim.Adam(0.0002, 0.5)\n\n#         # Build and compile the discriminator\n#         self.discriminator = self.build_discriminator()\n#         self.discriminator.compile(loss='binary_crossentropy',\n#                 optimizer=optimizer,\n#             metrics=['accuracy'])\n\n#         # Build and compile the generator\n#         self.generator = self.build_generator()\n#         self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n#         # The generator takes noise as input and generated imgs\n#         z = Input(shape=(100,))\n#         img = self.generator(z)\n\n#         # For the combined model we will only train the generator\n#         self.discriminator.trainable = False\n\n#         # The valid takes generated images as input and determines validity\n#         valid = self.discriminator(img)\n\n#         # The combined model  (stacked generator and discriminator) takes\n#         # noise as input => generates images => determines validity\n#         self.combined = Model(z, valid)\n#         self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n#     def build_generator(self):\n\n#         noise_shape = (100,)\n\n#         model = Sequential()\n\n#         model.add(Dense(256, input_shape=noise_shape))\n#         model.add(LeakyReLU(alpha=0.2))\n#         model.add(BatchNormalization(momentum=0.8))\n#         model.add(Dense(512))\n#         model.add(LeakyReLU(alpha=0.2))\n#         model.add(BatchNormalization(momentum=0.8))\n#         model.add(Dense(1024))\n#         model.add(LeakyReLU(alpha=0.2))\n#         model.add(BatchNormalization(momentum=0.8))\n#         model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n#         model.add(Reshape(self.img_shape))\n\n#         model.summary()\n\n#         noise = Input(shape=noise_shape)\n#         img = model(noise)\n\n#         return Model(noise, img)\n\n#     def build_discriminator(self):\n\n#         img_shape = (self.img_rows, self.img_cols, self.channels)\n\n#         model = Sequential()\n\n#         model.add(Flatten(input_shape=img_shape))\n#         model.add(Dense(512))\n#         model.add(LeakyReLU(alpha=0.2))\n#         model.add(Dense(256))\n#         model.add(LeakyReLU(alpha=0.2))\n#         model.add(Dense(1, activation='sigmoid'))\n#         model.summary()\n\n#         img = Input(shape=img_shape)\n#         validity = model(img)\n\n#         return Model(img, validity)\n\n#     def train(self, epochs, batch_size=128, save_interval=50):\n\n#         # Load the dataset\n#         X_train = data\n\n#         # Rescale -1 to 1\n#         X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n#         X_train = np.expand_dims(X_train, axis=3)\n\n#         half_batch = int(batch_size / 2)\n\n#         for epoch in range(epochs):\n\n#             # ---------------------\n#             #  Train Discriminator\n#             # ---------------------\n\n#             # Select a random half batch of images\n#             idx = np.random.randint(0, X_train.shape[0], half_batch)\n#             imgs = X_train[idx]\n\n#             noise = np.random.normal(0, 1, (half_batch, 100))\n\n#             # Generate a half batch of new images\n#             gen_imgs = self.generator.predict(noise)\n\n#             # Train the discriminator\n#             d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n#             d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n#             d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n\n#             # ---------------------\n#             #  Train Generator\n#             # ---------------------\n\n#             noise = np.random.normal(0, 1, (batch_size, 100))\n\n#             # The generator wants the discriminator to label the generated samples\n#             # as valid (ones)\n#             valid_y = np.array([1] * batch_size)\n\n#             # Train the generator\n#             g_loss = self.combined.train_on_batch(noise, valid_y)\n\n#             # Plot the progress\n#             print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n\n#             # If at save interval => save generated image samples\n#             if epoch % save_interval == 0:\n#                 self.save_imgs(epoch)\n\n#     def save_imgs(self, epoch):\n#         r, c = 5, 5\n#         noise = np.random.normal(0, 1, (r * c, 100))\n#         gen_imgs = self.generator.predict(noise)\n\n#         # Rescale images 0 - 1\n#         gen_imgs = 0.5 * gen_imgs + 0.5\n\n#         fig, axs = plt.subplots(r, c)\n#         cnt = 0\n#         for i in range(r):\n#             for j in range(c):\n#                 axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n#                 axs[i,j].axis('off')\n#                 cnt += 1\n#         fig.savefig(f\"../working/mnist_{epoch}.png\")\n#         plt.close()\n\n# gan = GAN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gan.train(epochs=30000, batch_size=32, save_interval=200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}